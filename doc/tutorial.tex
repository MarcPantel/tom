%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Some examples}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{A very simple example: Fibonacci}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This program is written in Java and use the \texttt{ATerm} library to
implement the term data structure.
The following example is an implementation fo the Fibonacci function,
using rewrite rules and Peano integers.

\noindent
The first part of the program contains some function and constant
definitions. We define:
\begin{itemize}
\item four \Java\ function symbols: \texttt{fzero, fsuc, fplus} and 
  \texttt{ffib}
\item a constant term: \texttt{tzero}
\item two \Java\ functions: \texttt{run} and \texttt{main}
\end{itemize}

\begin{verbatimwrite}{program.txt}
import jaterm.api.*;
import jaterm.shared.*;
import java.util.*;

public class Rule1 {

  private ATermFactory factory;
  
  private AFun fzero, fsuc, fplus,ffib;
  public ATermAppl tzero;

  public Rule1(ATermFactory factory) {
    this.factory = factory;
    fzero = factory.makeAFun("zero", 0, false);
    fsuc  = factory.makeAFun("suc" , 1, false);
    fplus = factory.makeAFun("plus", 2, false);
    ffib  = factory.makeAFun("fib" , 1, false);
    tzero = factory.makeAppl(fzero);
  }

  public void run(int n) {
    ATerm N = tzero;
    for(int i=0 ; i<n ; i++) {
      N = factory.makeAppl(fsuc,N);
    }
    ATerm res = null;
    for(int i=0 ; i<loop; i++) {
      res = fib(N);
    }
    System.out.println("result = " + res);
  }

  public final static void main(String[] args) {
    Rule1 test = new Rule1(new PureFactory(16));
    test.run(10);
  }
\end{verbatimwrite}
\programboxed{program.txt}

\noindent
The second part of the program defines the algebraic datatype
definition and specifies how terms are implemented. 
We define:
\begin{itemize}
\item the type \texttt{term}, which is implement by the \texttt{ATerm}
  \Java\ datatype 
\item four \TOM\ functions symbol: \texttt{zero, suc, plus} and 
  \texttt{fib} 
\end{itemize}

\begin{verbatimwrite}{program.txt}
  %typeterm term {
    implement { ATerm }
    get_fun_sym(t)      { (((ATermAppl)t).getAFun()) }
    cmp_fun_sym(t1,t2)  { t1 == t2 }
    get_subterm(t, n)   { (((ATermAppl)t).getArgument(n)) }
    equals(t1, t2)      { (t1.equals(t2)) }
  }

  %op term zero {
    fsym { fzero }
    make { factory.makeAppl(fzero) }
  }
  
  %op term suc(term) {
    fsym { fsuc }
    make(t) { factory.makeAppl(fsuc,t) }
  }
  
  %op term plus(term,term) {
    fsym { fplus }
    make(t1,t2) { factory.makeAppl(fplus,t1,t2) }
  }
  
  %op term fib(term) {
    fsym { ffib }
    make(t) { factory.makeAppl(ffib,t) }
  }
\end{verbatimwrite}
\programboxed{program.txt}

\noindent
The last part of the program specifies two rewrite systems using the
\texttt{\%rule} construct:
\begin{verbatimwrite}{program.txt}
  %rule {
    plus(x,zero)   -> x
    plus(x,suc(y)) -> suc(plus(x,y))
  }

  %rule {
    fib(zero) -> suc(zero)
    fib(suc(zero)) -> suc(zero)
    fib(suc(suc(x))) -> plus(fib(x),fib(suc(x)))
  }
}
\end{verbatimwrite}
\programboxed{program.txt}

\subsection{A funny example: Addition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this example written in \C, we want to define the integer addition.
On one side we use the Peano axiomatization to specifies the addition
function. On the other side, we use \C\ builtin integer \texttt{int}
to implement the algebraic datatype.

\noindent
In the first part of the program, we define the mapping between the
algebraci datatype and its implementation:
\begin{itemize}
\item the type \texttt{Nat} is implemented by the \texttt{int} \C\
  type 
\item the root symbol of a \texttt{Nat} is a successor \texttt{suc} is
  this natural is not zero
\item the subterm of a natural is a natural that corresponds
  to its predecessor (i.e. \texttt{n-1})
\end{itemize}


\begin{verbatimwrite}{program.txt}
#include<stdio.h>

#define ZERO  0
#define SUC   1

%typeterm Nat {
  implement { int }
  get_fun_sym(i)      { ((i==0)?ZERO:SUC) }
  cmp_fun_sym(t1,t2)  { (t1 == t2) }
  get_subterm(i, n)   { (i-1) }
}
%op Nat zero {
  fsym { ZERO }
}
%op Nat suc(Nat) {
  fsym { SUC }
}
\end{verbatimwrite}
\programboxed{program.txt}

\noindent
The second part of this program defines:
\begin{itemize}
\item the \texttt{suc} function, that returns the successor of a given integer
\item the \texttt{plus} function, specified over the algebraic datatype
\item the \texttt{main} function
\end{itemize}
The user should note that the computation is done by pattern matching,
but the involved terms are always represented (in memory) by a builtin
\texttt{int}.


\begin{verbatimwrite}{program.txt}
int suc(int t) {
  return t+1;
}

int plus(int t1, int t2) {
  %match(Nat t1, Nat t2) {
    x,zero   -> { return x; }
    x,suc(y) -> { return suc(plus(x,y)); }
  }
}

void main() {
  printf("res = %d\n", plus(10,10) );
}
\end{verbatimwrite}
\programboxed{program.txt}

\subsection{A list matching example: Sorting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following example illustrate how list matching can be used to
implement a sort algorithm in a concise way.
For simplicity, we only present parts of the program that are related
to \TOM.

\noindent
In the first part, we define a type~\texttt{L}, using the
\texttt{\%typelist} construct. This datatype is implemented by an
\texttt{ATermList} (defined in the ATerm library).
By convention, the root symbol of any term of sort~\texttt{L} is the
\texttt{conc} function symbol. In the last part of the
\texttt{\%typelist} construct, we explain to \TOM\ how to acces to the
head, the tail and how to know if a list (of sort~\texttt{L}) is
empty.

\begin{verbatimwrite}{program.txt}
  %typelist L {
    implement { ATermList }
    get_fun_sym(t) { ((t instanceof ATermList)?
                       factory.makeAFun("conc", 1, false):null) }
    cmp_fun_sym(t1,t2) { t1 == t2 }
    equals(l1,l2)  { l1.equals(l2) }
    get_head(l)    { ((ATermList)l).getFirst() }
    get_tail(l)    { ((ATermList)l).getNext() }
    is_empty(l)    { ((ATermList)l).isEmpty() }
  }
\end{verbatimwrite}
\programboxed{program.txt}

\noindent
Then, we specify how the \texttt{conc} function symbol is
implemented. In addition to the classical \texttt{\%type} construct,
we have to specify how to build an empty list, and how to insert an 
element to a list.

\begin{verbatimwrite}{program.txt}
  %oplist L conc( E* ) {
    fsym { factory.makeAFun("conc", 1, false) }
    make_empty()  { factory.makeList() }
    make_add(l,e) { ((ATermList)l).insert((ATerm)e) }
  }
\end{verbatimwrite}
\programboxed{program.txt}

\noindent
In this second part, we define the sort of the elements:~\texttt{E},
and we define three constants: \texttt{a}, \texttt{b} and \texttt{c}

\begin{verbatimwrite}{program.txt}
  %typeterm E {
    implement { ATerm }
    get_fun_sym(t)      { (((ATermAppl)t).getAFun()) }
    get_subterm(t, n)   { (((ATermAppl)t).getArgument(n)) }
    equals(t1, t2)      { (t1.equals(t2)) }
  }

  %op E a { fsym { factory.makeAFun("a", 0, false) } }
  %op E b { fsym { factory.makeAFun("b", 0, false) } }
  %op E c { fsym { factory.makeAFun("c", 0, false) } }
\end{verbatimwrite}
\programboxed{program.txt}

\noindent
In the last part of the program, we specify the sorting algorithm,
using only one rule.
The notation \texttt{conc(X1*,x,X2*,y,X3*)} means that:
\begin{itemize}
\item the subject should have the \texttt{conc} symbol as the root
  symbol

\item the variables \texttt{X1*}, \texttt{X2*} and \texttt{X3*} can be
  instantiated by the empty list (the '\texttt{*}' after a variable
  name means that it is a list variable the sort of the root
  symbol:~\texttt{L} in this example) 

\item the variable \texttt{x} and \texttt{y} are as usual and has to
  be instantiate by a term of sort~\texttt{E}, the sort of subterms of
  \texttt{conc} 
\end{itemize}
The introduced variables may of course be used in the semantic action
part:
\begin{itemize}
\item first, we collect the name of the elements referenced by
  \texttt{x} and \texttt{y}.

\item the name are compared accoring to the \Java\ string
  lexicographic order (note that it should have been possible to
  implement our lexicographic order if needed).

\item if the condition is not satisfied (i.e. \texttt{x} is not
  greater than \texttt{y}), we do nothing. This means that another
  substitution is asked. If there is no more available subtitution,
  the list is sorted.

\item if the condition is satified, we have found an \texttt{x}
  greater than a \texttt{y}: we have to swap their value and run the
  sorting algorithm again.
\end{itemize}

\begin{verbatimwrite}{program.txt}  
  public ATermList sort1(ATermList l) {
    %match(L l) {
      conc(X1*,x,X2*,y,X3*) -> {
        String xname = ((ATermAppl)x).getName();
        String yname = ((ATermAppl)y).getName();
        if(xname.compareTo(yname) > 0) {
          return sort1( X1.append(y).concat(X2).append(x).concat(X3) );
        }
      }
      _ -> { return l; }
    }
  }
\end{verbatimwrite}
\programboxed{program.txt}

\subsection{A more complex example: Object simplification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The goal of this example is to show how \TOM\ can be used to implement
a simplification function that directly works on objects (and not on
classical terms). 
To illustrate this purpose, we consider a simple hierarchy of classes
that represents symbolic expressions such as constants, unary operators
and binary operators:

\begin{verbatimwrite}{program.txt}
import java.util.*;
public class Record4 {
  abstract class Exp {
    public abstract String getOperator();
  }
  
  class CstExp extends Exp {
    public Object value;
    public CstExp(Object value) {
      this.value = value;
    }
    public String getOperator() {
      return "" + value;
    }
  }

  class IntExp extends CstExp {
    public IntExp(int value) {
      super(new Integer(value));
    }
  }
  
  class StringExp extends CstExp {
    public StringExp(String value) {
      super(value);
    }
  }
\end{verbatimwrite}
\programboxed{program.txt}


\begin{verbatimwrite}{program.txt}  
  class UnaryOperator extends Exp {
    public Exp first;
    public UnaryOperator(Exp first) {
      this.first = first;
    } 
    public String getOperator() { return ""; }
  }

  class BinaryOperator extends Exp {
    public Exp first;
    public Exp second;
    public BinaryOperator(Exp first, Exp second) {
      this.first = first;
      this.second = second;
    }
    public String getOperator() { return ""; }
  }

  class Plus extends BinaryOperator {
    public Plus(Exp first, Exp second) {
      super(first,second);
    }
    public String getOperator() { return "Plus"; }
  }

  class Mult extends BinaryOperator {
    public Mult(Exp first, Exp second) {
      super(first,second);
    }
    public String getOperator() { return "Mult"; }
  }
  
  class Uminus extends UnaryOperator {
    public Uminus(Exp first) {
      super(first);
    }
    public String getOperator() { return "Uminus"; }
  }
\end{verbatimwrite}
\programboxed{program.txt}

\noindent
In the second part of the program, we define the mapping between the
object model and the algebraic datatype framework.
\Java\ objects of sort \texttt{Object} are mapped to the \TOM\ type
\texttt{TomObject}, \Java\ objects of sort \texttt{Exp} are
mapped to \texttt{TomExp}, and \Java\ objects of sort \texttt{Integer} are
mapped to \texttt{TomInteger}.
In the two first cases, the root symbol of terms of such a type
corresponds to the dynamic type of the corresponding \Java\ object
(the root symbol of a \texttt{TomExp} term is the dynamic type of the
object of sort \texttt{Exp}). 
Comparing two function symbol in the algebraic framework,
corresponds to check if a given dynamic type is assignable from
another one.

\begin{verbatimwrite}{program.txt}
  %typeterm TomObject {
    implement           { Object }
    get_fun_sym(t)      { t.getClass() }
    cmp_fun_sym(subjectFunSym,patternFunSym)  {
      ((Class)patternFunSym).isAssignableFrom(((Class)subjectFunSym))
    }
  }

  %typeterm TomExp {
    implement           { Exp }
    get_fun_sym(t)      { t.getClass() }
    cmp_fun_sym(subjectFunSym,patternFunSym)  {
      ((Class)patternFunSym).isAssignableFrom(((Class)subjectFunSym))
    }
  }

  private final static Integer ZERO = new Integer(0);
  private final static Integer SUC  = new Integer(1);
  %typeterm TomInteger {
    implement { Integer }
    get_fun_sym(i)      { (((Integer)i).intValue()==0)?ZERO:SUC }
    cmp_fun_sym(i1,i2)  { i1 == i2 }
    get_subterm(i, n)   { new Integer(((Integer)i).intValue()-1) }
  }


\end{verbatimwrite}
\programboxed{program.txt}

\noindent
The second part of the program defines an algebraic function symbols
for each class defined in the object model. We also define the mapping
between instance variables and subterms of such function symbols (this
is done by the \texttt{get\_slot} \TOM\ construct.

\begin{verbatimwrite}{program.txt}
  %op TomExp BinaryOperator(first:TomExp, second:TomExp) {
    fsym { (new BinaryOperator(null,null)).getClass() }
    get_slot(first,t) { ((BinaryOperator)t).first }
    get_slot(second,t) { ((BinaryOperator)t).second }
  }

  %op TomExp UnaryOperator(first:TomExp) {
    fsym { (new UnaryOperator(null)).getClass() }
    get_slot(first,t) { ((UnaryOperator)t).first }
  }

  %op TomExp Plus(first:TomExp, second:TomExp) {
    fsym { (new Plus(null,null)).getClass() }
    get_slot(first,t) { ((Plus)t).first }
    get_slot(second,t) { ((Plus)t).second }
  }

  %op TomExp Mult(first:TomExp, second:TomExp) {
    fsym { (new Mult(null,null)).getClass() }
    get_slot(first,t) { ((Mult)t).first }
    get_slot(second,t) { ((Mult)t).second }
  }

  %op TomExp Uminus(first:TomExp) {
    fsym { (new Uminus(null)).getClass() }
    get_slot(first,t) { ((Uminus)t).first }
  }

  %op TomExp CstExp(value:TomObject) {
    fsym { (new CstExp(null)).getClass() }
    get_slot(value,t) { ((CstExp)t).value }
  }

  %op TomExp IntExp(value:TomInteger) {
    fsym { (new IntExp(0)).getClass() }
    get_slot(value,t) { ((IntExp)t).value }
  }

  %op TomInteger zero {
    fsym { ZERO }
  }

  %op TomInteger suc(TomInteger) {
    fsym { SUC }
  }
\end{verbatimwrite}
\programboxed{program.txt}


\noindent
In the third part of the program, we specify how to simplify an
expression of sort \texttt{Exp}.
In order to do this, we have to define a traversal function
(texttt{traversalSimplify}) that goes through a term and recursivly
applies the \texttt{simplify} function. 
This is easilly done by pattern matching:

\begin{verbatimwrite}{program.txt}  
  ...
  public Exp buildExp() {
    return new Mult(new Plus(new StringExp("a"), new IntExp(0)), new IntExp(1));
  }

  public Exp traversalSimplify(Exp t) {
    %match(TomExp t) {
      UnaryOperator[first=e1] -> {
        ((UnaryOperator)t).first  = traversalSimplify(e1);
        return simplify(t);
      }
      
      BinaryOperator[first=e1, second=e2] -> {
        ((BinaryOperator)t).first  = traversalSimplify(e1);
        ((BinaryOperator)t).second = traversalSimplify(e2);
        return simplify(t);
      }
    }
    return t;
  }
\end{verbatimwrite}
\programboxed{program.txt}

\noindent
Before presenting the \texttt{simplify} function, the reader should
note that the pattern matching is done modulo the dynamic type of
objects. This means that the pattern \texttt{BinaryOperator[first=e1,
  second=e2]} can effectively match a term rooted by a \texttt{Plus}
symbol for example. This is true because the class
\texttt{BinaryOperator} is assignable from the class \texttt{Plus}.

\noindent
The \texttt{simplify} function describes the simplification rules:
\begin{itemize}
\item the addition of two \texttt{IntExp} can be simplified a
  \texttt{IntExp} whose value is the sum of the two previous values.
\item the addition of an expression with the neutral element
  \texttt{zero} can be simplified into the first expression
\item ...
\end{itemize}
And last, we have to define the \texttt{myEqual} functions to check if
two expression are equals.


\begin{verbatimwrite}{program.txt}
  public Exp simplify(Exp t) {
    %match(TomExp t) {
      Plus[first=IntExp(v1), second=IntExp(v2)] -> {
        return new IntExp(v1.intValue() + v2.intValue());
      }

      Plus[first=e1, second=IntExp(zero)] -> { return e1; }
      Plus[second=e1, first=IntExp(zero)] -> { return e1; }

      Plus[first=e1, second=Uminus(e2)] -> {
        if(myEquals(e1,e2)) {
          return new IntExp(0);
        } else {
          return t;
        }
      }

      Mult[first=IntExp(v1), second=IntExp(v2)] -> {
        return new IntExp(v1.intValue() * v2.intValue());
      }
      
      Mult[first=e1, second=IntExp(suc(zero))] -> { return e1; }
      Mult[second=e1, first=IntExp(suc(zero))] -> { return e1; }
    }
    return t;
  }

  public boolean myEquals(Exp t1, Exp t2) {
    %match(TomExp t1, TomExp t2) {
      
      CstExp(e1), CstExp(e2)       -> { return e1.equals(e2); }
      
      UnaryOperator[first=e1], UnaryOperator[first=f1] -> {
        return t1.getOperator().equals(t2.getOperator()) && myEquals(e1,f1);
      }
      
      BinaryOperator[first=e1, second=e2], BinaryOperator[first=f1, second=f2] -> {
        return t1.getOperator().equals(t2.getOperator()) &&
               myEquals(e1,f1) && myEquals(e2,f2);
      }

    }
    return false;
  }
}
\end{verbatimwrite}
\programboxed{program.txt}

\newpage
\section{\TOM: A programming language}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As mentioned previously, \TOM\ should not be seen as a programming
language. It is rather an ``assembly language'' for implementing
rule-based programming languages. 
However, \TOM\ has been designed to be simple and clear enough
to be used by a human. It is also sufficiently flexible and powerful
to support the development of several extensions.
It may be ambitious to pretend that the conception of \TOM\ reminds
the conception of \TeX, but here is the analogy:
\begin{itemize}
\item \TeX\ is simple enough to write a book: the {\TeX}book is
  written in \TeX.
  Similarly, \TOM\ is simple enough to write a compiler: the \TOM\
  compiler is written in \Java+\TOM.
\item \TeX\ is powelful enough to implement \LaTeX. 
  Similarly, \TOM\ should be able to support the development of
  several extensions such as specialized of \TOM\ for \Java\ or \Eiffel.
\end{itemize}

The current version of \TOM\ only supports \C\ and \Java\ as target
language, but we are currently working on a new back-end for the
\Eiffel\ language.

Because \TOM\ ought to support several languages, its syntax may
not necessary be well integrated in the target language. The use of
braces `\texttt{\{}' is not familiar to \Eiffel\ programmers for instance.
For these reasons, we also consider to develop specific language
extensions such as a specialized version of \TOM\ for \Java\ or \Eiffel\
for example. 
For a given target language, we only have to develop a parser extended
with the \TOM\ constructs.
The main advantage of this approach is that \TOM\ still remains the
main underlying compiler: the compilation algorithms are written only
once, and the term data-structure can be easily modified according to
the developped application (only the generic API has to be modified).

In this section we present the capabilities and the advantages
offered by \TOM\ wtr. to other similar approaches.

\subsection{Algebraic specification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
When using a classical imperative programming language, the need of
a formal algebraic specification may appear in at least two
situations: 
\begin{itemize}
\item when transforming a complex tree based data structure
\item when describing a complex transformation process
\end{itemize}

The first case often occurs when performing program transformations or
document manipulations, such as XML transformations for instance.
A compiler is a good example of application that performs program
transformation: compiling a program is nothing else than transforming
an abstract syntax tree in another one.   
In general, the abstract syntax tree of a program is a complex tree
based data structure that contains several kinds of nodes.
For these situations, the expressiveness of pattern matching is
interesting to shortly specify the transformation process.

The second case may occur when performing complex symbolic simplification
for example. 
In theory, any sufficiently poweful programming language can be used
to express a complex symbolic transformation. But in practice, 
the probability of introducing a mistake is usually inversally
proportional to the expressiveness of the used language.
The main advantage of using an algebraic specification language to
express such algorithms is that we can automatically check some
properties.
Specifying by rewrite rules rougthly consists in describing the
algorithm by cases. By using a sufficient completeness definition
checker for example, it becomes possible to detect if a case has been
forgotten for example. 
We may also use some more elaborated provers, such as the Knuth-Bendix completion, to
prove the termination and the confluence of the rewrite system for
example.

The \TOM\ compiler contains a sufficient completeness definition
checker that only inspects the left-hand side of each rewrite rule.
When compiling a \texttt{\%match} or a \texttt{\%rule} construct, this
static analysis tool detects if the case-definition is not complete:
if there exists a ground term such that no pattern can match it.
The static analysis tool can also detect if a pattern is
redondant: in \TOM, rules are applied with priority according to the
textual order. So, if a pattern is followed by one of its instances,
the latter instance can never be matched. 
Consider the following rewrite system:
\begin{verbatim}
  %rule {
    f(g(x)) -> x
    f(g(a)) -> b
  }
\end{verbatim}

The \TOM\ checker can detect two incompleteness:
\begin{itemize}
\item the second rule can never be applied because any term matched by
  the second rule is also matched by the first one
\item the function definition is not exhaustive since there is at least
  one term that cannot by matched by the system (the term $f(a)$ for
  instance). 
\end{itemize}

% \begin{itemize}
% \item expressiveness
% \item verification
% \end{itemize}

\subsection{List matching}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\subsection{Object rewriting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the object-oriented paradigm, classes are containers that control
the access to some data. 
An application is a set of tasks that naturally manipulate this data.
The classical design of an application consists in extending 
the container classes with methods corresponding to the tasks to
achieve, but that is only feasible if the number of tasks is very few.
When the number of tasks increases, the classes quickly become less
focused on their real tasks and more and more tailored for that
special application. This means it becomes harder to use them  in
other applications.  

When the task involves traversing a tree, a parse tree consisting of
various kinds of nodes for example, the problem becomes even
worse. Not only is it bad for the node classes, as they must know
about all operations that can be done  on the tree, it is also bad for
the tasks as their code is scattered all over the program. 

The purpose of the Visitor Pattern is to encapsulate an operation that
we want to perform on the elements of a data structure. In this
way, we can change the operation being performed on a structure
without the need of changing the classes of the elements that we are
operating on. Using a Visitor pattern allows us to decouple the
classes for the data structure and the algorithms used upon them.

To implement this pattern, each node in the data structure ``accepts''
a Visitor, which sends a message to the Visitor which includes the
node's class. The visitor will then execute its algorithm for that
element. This process is known as ``double dispatching''. The node makes
a call to the Visitor, passing itself in, and the Visitor executes its
algorithm on the node. In double dispatching, the call made depends
upon the type of the Visitor and of the Host (data structure node),
not just of one component. 

One key advantage of using the Visitor Pattern is that adding new
operations to perform upon the data structure is very easy. 
We only have to create a new Visitor and define the operation there.
This is the result of the very distinct separation of variant and
invariant behavior in the Visitor pattern.   
The invariant behaviors are represented by the data structure elements
and the abstract Visitor. The variant behaviors are encapsulated in
the concrete Visitors. 

In one sense, specifying by rewrite rules is very closed to the
object-oriented paradigm and the Visitor design pattern:  
\begin{itemize}
\item the data-structure is defined by a set of sorts and constructors 
  instead of a set of classes and instance variables
\item the operations are described by rewrite rules instead of
  Visitors: we can change the operation being performed on a structure
  without the need of changing the signature.
\end{itemize}

One particularity of rewriting is the presence of pattern matching:
this allows to deeply inspect an object in a concise way.
When writting a compiler for example, it is nice to perform static
analysis to partially evaluate constant expressions at compile time.
A partial evaluator is a set of simplification rules of the form:
``the addition of two constant integers can be simplified into their
sum'', ``the addition of the constant zero to an expression can be
simplified into the expression'', \textit{etc.} 

Implementing such a procedure in an object-oriented paradigm is not so
straightforward. Suppose that we have the following hierarchy of classes
\texttt{Expression}, \texttt{BinaryExpression},
\texttt{PlusExpression}, \texttt{ConstantExpression},
\texttt{IntegerExpression}, \textit{etc.} 
Given an instance of \texttt{PlusExpression}, to implement the first
rule we need to use dynamic typing or a double dispatching to check
that the two subtems are instances of \texttt{IntegerExpression}. 

Given a adequate mapping from objets to terms, the two previously
rewrite rules can be easilly encoded in \TOM:
\begin{verbatim}
  %rule {
    PlusExpression(ConstantExpression(IntegerExpression(x)),
                   ConstantExpression(IntegerExpression(y)))
    -> ConstantExpression(IntegerExpression(x+y))

    PlusExpression(ConstantExpression(IntegerExpression(zero)),exp) -> exp
    PlusExpression(ConstantExpression(exp,IntegerExpression(zero))) -> exp
  }
\end{verbatim}


% \begin{itemize}
% \item comparison with the visitor pattern
% \end{itemize}


\subsection{Comparison with similar approaches}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Several systems have been developed in order to integrate pattern
matching and transformation facilities into imperative languages. 
Let us mention for instance \textsf{Rigal}~\cite{AugustonMi1990a,Rigal},
\textsf{R++}~\cite{CrawfordDLMP-AAAI96,Rpp}, 
\textsf{App}~\cite{App}, \textsf{Prop}~\cite{Prop} and
\textsf{Pizza}~\cite{}. 

Each of these systems has its own specificity. 
\textsf{Rigal} is presented as
a compiler construction language based on advanced pattern matching.
\textsf{R++} and \textsf{App} are pre-processors for \Cplusplus: the first
one adds production rule constructs to \Cplusplus, whereas the second one
extends \Cplusplus\ with a match construct. \textsf{Prop} is a multi-paradigm
extension of \Cplusplus, including pattern matching constructs.
Finally, \textsf{Pizza} is a \Java\ extension that supports 
parametric polymorphism, first-class functions, class cases and
pattern matching. 
All these approaches are interesting and propose some very powerful
constructs, but in a certain sense, they are too powerful and less
generic than \TOM. 
In the spirit, \textsf{Prop} and \textsf{Pizza} are very closed to
\TOM: they add pattern matching facilities to a classical imperative
language, but the approach is not similar.
\textsf{Prop} and \textsf{Pizza} are not preprocessors: they really
extend \Cplusplus\ and \Java\ with some new pattern matching
constructions. In one sense, the integration is better and more
transparent, on the other side, the term data structure cannot be
parametrized by the user: the pattern matching process can only act on 
internal data-structures.  
Consequently, these systems cannot be used to add pattern matching
facilities in an existing project written for example in \C,
\Cplusplus\ or \Java. This may be a drawback because it is not easy a
user to use rule-based programming style if the first thing to do
consists in translating the existing main data-structures.
